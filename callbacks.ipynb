{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOchW97XVDhSCtAHS1IANvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skevin-dev/DataAnalysis/blob/main/callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the training of our model, we can use various callbacks. Callbacks allow us to customize and control the training process in fine-grained ways. We'll implement three key callbacks:\n",
        "        \n",
        "- **Learning Rate Scheduling**: Adjusts the learning rate over time, which can lead to better model performance.\n",
        "- **Early Stopping**: Halts training when the model's performance stops improving, which prevents overfitting. We'll stop if validation loss doesn't improve for at least 5 epochs.\n",
        "- **Checkpointing**: Saves the model every time validation loss gets better than in the epoch prior. This allows us to recover the best model once training completes.\n",
        "\n",
        "In order to use these callbacks, we need to implement them and then update the `train` function.\n",
        "\n",
        "For the Learning Rate Scheduling, we'll use `StepLR` from `torch.optim`. The `StepLR` scheduler decays the learning rate by multiplicative factor `gamma` every `step_size` epochs."
      ],
      "metadata": {
        "id": "lq0YBAk_Skf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Period of learning rate decay\n",
        "step_size = 4\n",
        "# Multiplicative factor of learning rate decay\n",
        "gamma = 0.2\n",
        "\n",
        "# Initialize the learning rate scheduler\n",
        "scheduler = StepLR(\n",
        "    optimizer,\n",
        "    step_size=step_size,\n",
        "    gamma=gamma,\n",
        ")\n",
        "\n",
        "print(type(scheduler))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "dHJFSRFqSlPu",
        "outputId": "42e7b465-4c04-4f16-ee49-ccd98df563bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'StepLR' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04bd69a287a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Initialize the learning rate scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m scheduler = StepLR(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'StepLR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def early_stopping(validation_loss, best_val_loss, counter):\n",
        "    \"\"\"Function that implements Early Stopping\"\"\"\n",
        "\n",
        "    stop = False\n",
        "\n",
        "    if validation_loss < best_val_loss:\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "    # Check if counter is >= patience (5 epochs in our case)\n",
        "    # Set stop variable accordingly\n",
        "    if counter >=5:\n",
        "        stop = True\n",
        "\n",
        "    return counter, stop"
      ],
      "metadata": {
        "id": "BZi3GTmJSsXk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkpointing(validation_loss, best_val_loss, model, optimizer, save_path):\n",
        "\n",
        "    if validation_loss < best_val_loss:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": best_val_loss,\n",
        "            },\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"Checkpoint saved with validation loss {validation_loss:.4f}\")"
      ],
      "metadata": {
        "id": "LAxQx7dfS5J1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from training import score, train_epoch\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    optimizer,\n",
        "    loss_fn,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=20,\n",
        "    device=\"cpu\",\n",
        "    scheduler=None,\n",
        "    checkpoint_path=None,\n",
        "    early_stopping=None,\n",
        "):\n",
        "    # Track the model progress over epochs\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    learning_rates = []\n",
        "\n",
        "    # Create the trackers if needed for checkpointing and early stopping\n",
        "    best_val_loss = float(\"inf\")\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    print(\"Model evaluation before start of training...\")\n",
        "    # Test on training set\n",
        "    train_loss, train_accuracy = score(model, train_loader, loss_fn, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    # Test on validation set\n",
        "    validation_loss, validation_accuracy = score(model, val_loader, loss_fn, device)\n",
        "    val_losses.append(validation_loss)\n",
        "    val_accuracies.append(validation_accuracy)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(\"\\n\")\n",
        "        print(f\"Starting epoch {epoch}/{epochs}\")\n",
        "\n",
        "        # Train one epoch\n",
        "        train_epoch(model, optimizer, loss_fn, train_loader, device)\n",
        "\n",
        "        # Evaluate training results\n",
        "        train_loss, train_accuracy = score(model, train_loader, loss_fn, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Test on validation set\n",
        "        validation_loss, validation_accuracy = score(model, val_loader, loss_fn, device)\n",
        "        val_losses.append(validation_loss)\n",
        "        val_accuracies.append(validation_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.4f}%\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print(f\"Validation accuracy: {validation_accuracy*100:.4f}%\")\n",
        "\n",
        "        # # Log the learning rate and have the scheduler adjust it\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        learning_rates.append(lr)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Checkpointing saves the model if current model is better than best so far\n",
        "        if checkpoint_path:\n",
        "            checkpointing(\n",
        "                validation_loss, best_val_loss, model, optimizer, checkpoint_path\n",
        "            )\n",
        "\n",
        "        # Early Stopping\n",
        "        if early_stopping:\n",
        "            early_stopping_counter, stop = early_stopping(\n",
        "                validation_loss, best_val_loss, early_stopping_counter\n",
        "            )\n",
        "            if stop:\n",
        "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "                break\n",
        "\n",
        "        if validation_loss < best_val_loss:\n",
        "            best_val_loss = validation_loss\n",
        "\n",
        "    return (\n",
        "        learning_rates,\n",
        "        train_losses,\n",
        "        val_losses,\n",
        "        train_accuracies,\n",
        "        val_accuracies,\n",
        "        epoch,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "htJ3zBicTaOO",
        "outputId": "d2086936-fe19-428a-f8c3-ea85357b8638"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'training'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1220978b0718>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m def train(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'training'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npFlmOTDTi7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The competition requires us to save the model predictions as a CSV file. The first column should be called ID and contains the image filename. The rest of the columns should be labeled by the class name.\n",
        "\n",
        "In order to get predicted probabilities of our model, we'll create a function `file_to_confidence` which is similar to what we created for this purpose in Project 1. The function makes model predictions on a single image. The steps in the function are:\n",
        "- Open the image.\n",
        "- Apply our transformation pipeline to the image as our model expects.\n",
        "- Use `unsqueeze` to change the image tensor to 4D ($1$ x $3$ x $224$ x $224$) as our model is expecting a batch of images.\n",
        "- Place image on device we're using.\n",
        "- Make prediction and pass it through a `SoftMax` to get probabilities (numbers between $0$ and $1$, that sum to $1$).\n",
        "- Convert result to a DataFrame"
      ],
      "metadata": {
        "id": "nPvDB22UT95_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "\n",
        "def file_to_confidence(model, datadir, filename, transform_pipeline):\n",
        "    file_path = os.path.join(datadir, filename)\n",
        "    image = PIL.Image.open(file_path)\n",
        "    transformed = transform_pipeline(image)\n",
        "    unsqueezed = transformed.unsqueeze(0)\n",
        "    image_cuda = unsqueezed.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        model_raw = model(image_cuda)\n",
        "        confidence = torch.nn.functional.softmax(model_raw, dim=1)\n",
        "\n",
        "    conf_df = pd.DataFrame([[filename] + confidence.tolist()[0]])\n",
        "    conf_df.columns = [\"ID\"] + train_dataset.dataset.classes\n",
        "\n",
        "    return conf_df"
      ],
      "metadata": {
        "id": "89eAvxJ_Tj6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_dfs = []\n",
        "\n",
        "for filename in tqdm(os.listdir(test_dir), desc=\"Predicting on test set\"):\n",
        "    small_dfs.append(\n",
        "        file_to_confidence(model, test_dir, filename, transform_normalized)\n",
        "    )\n",
        "\n",
        "confidence_df = pd.concat(small_dfs)\n",
        "\n",
        "confidence_df = confidence_df.sort_values(\"ID\").reset_index(drop=True)\n",
        "confidence_df.head()"
      ],
      "metadata": {
        "id": "ynwbmoCVULD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}